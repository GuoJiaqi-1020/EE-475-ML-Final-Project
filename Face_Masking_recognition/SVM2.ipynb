{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import hessian\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel:\n",
    "    def __init__(self,name,**kwargs):         \n",
    "        # define kernel            \n",
    "        if name == 'polys':\n",
    "            self.kernel = self.kernel_poly\n",
    "            \n",
    "        if name == 'fourier':\n",
    "            self.kernel = self.kernel_fourier   \n",
    "\n",
    "        if name == 'gaussian':\n",
    "            self.kernel = self.kernel_gaussian   \n",
    "            \n",
    "        ### set hyperparameters of kernel ###\n",
    "        # degree of polynomial\n",
    "        self.D = 2\n",
    "        if 'degree' in kwargs:\n",
    "            self.D = kwargs['degree']\n",
    "            \n",
    "        self.beta = 0.01\n",
    "        if 'beta' in kwargs:\n",
    "            self.beta = kwargs['beta']\n",
    "            \n",
    "    # poly kernel\n",
    "    def kernel_poly(self,x1,x2):    \n",
    "        H = (1 + np.dot(x1.T,x2))**self.D - 1\n",
    "        return H.T\n",
    "    \n",
    "    # fourier kernel\n",
    "    def kernel_fourier(self,x1,x2):    \n",
    "        # loop over both matrices and create fourier kernel\n",
    "        num_cols1 = x1.shape[1]\n",
    "        num_cols2 = x2.shape[1]\n",
    "        H = np.zeros((num_cols1,num_cols2))\n",
    "        for n in range(num_cols1):\n",
    "            for m in range(num_cols2):\n",
    "                val = np.pi*(x1[:,n] - x2[:,m])                \n",
    "                ind = np.argwhere(val == 0)\n",
    "                val[ind] += 10**(-10)\n",
    "                val1 = np.sin((2*self.D + 1)*val)\n",
    "                val2 = np.sin(val)\n",
    "                val3 = np.prod(val1/val2,0) - 1\n",
    "                H[n,m] = val3\n",
    "        return H.T\n",
    "    \n",
    "    # gaussian kernel\n",
    "    def kernel_gaussian(self,x1,x2):  \n",
    "        dist = cdist(x1.T, x2.T, metric='euclidean')**2\n",
    "        H = np.exp(-self.beta*dist)\n",
    "        return H.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load(path, norm=False):\n",
    "    data = np.loadtxt(path, delimiter=',')\n",
    "    print(data.shape)\n",
    "    data = clean(data)\n",
    "    x = data[:-1, :]\n",
    "    if norm is True:\n",
    "        x = normalize(x)\n",
    "    y = data[-1:,:]\n",
    "    return x,y\n",
    "\n",
    "def normalize(x):\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        mean = np.mean(x[i])\n",
    "        dev = np.sqrt(np.mean(np.square(x[i]-mean)))\n",
    "        x[i] = (x[i] - mean) / dev\n",
    "    return x\n",
    "\n",
    "def clean(data):\n",
    "    return data[:,~np.isnan(data).any(axis=0)]  # delete all the coloum which has nan\n",
    "\n",
    "\n",
    "class SVMMultiClass_Model:\n",
    "    def __init__(self, x, y, num_class, kernel_name = 'gaussian', **kwargs):       # x: [features, batch]  y: [1, batch]\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.num_class = num_class\n",
    "        self.kernel = Kernel(kernel_name, **kwargs)\n",
    "        self.K_train = self.kernel.kernel(self.x,self.x)\n",
    "        self.w = np.random.random((len(self.K_train)+1,num_class))*2 - 1       # w: [features, class]\n",
    "\n",
    "\n",
    "        pass\n",
    "    def linear_model(self, w):\n",
    "        return np.dot(self.K_train.T, w[1:,:]) + w[0,:]      # [batch, class]\n",
    "\n",
    "    def linear_model2(self,x,w):                       \n",
    "        return np.dot(x.T, w[1:]) + w[0]\n",
    "\n",
    "    def predict_model(self, x):\n",
    "        return np.dot(self.kernel.kernel(self.x,x).T, self.w[1:,:]) + self.w[0,:]\n",
    "\n",
    "    def normalize_weight(self, w):\n",
    "        '''\n",
    "        nofrmailze the weight for signed distance\n",
    "        w_normalized = w_origin / sqrt(sum(w_origin^2)) for each class\n",
    "        '''   \n",
    "        for c in range(len(w[0])):    # loop class\n",
    "            sum = np.sqrt(np.sum(np.square(w[:,c])))   # L2 norm\n",
    "            w[:,c] = w[:,c] / sum\n",
    "        return w\n",
    "\n",
    "    def max_distance(self, w):\n",
    "        max_class = np.argmax(self.linear_model(w), axis = 1)     #[batch] \n",
    "        max_dis = np.max(self.linear_model(w), axis = 1)          #[batch]\n",
    "        return max_class, max_dis\n",
    "\n",
    "    def perceptron_multiclass(self, w):\n",
    "        # w = self.normalize_weight(w)\n",
    "        cost = 0\n",
    "        max_class, max_dis = self.max_distance(w)\n",
    "        for i in range(len(self.x[0])):\n",
    "            cost += max_dis[i] - (np.dot(self.x[:,i], w[1:,int(self.y[0,i])]) + w[0, int(self.y[0,i])])\n",
    "        cost = cost / len(self.x[0])\n",
    "        return cost\n",
    "    \n",
    "    def softmax_multiclass(self, w):\n",
    "        cost = 0\n",
    "        for i in range(len(self.y[0])):\n",
    "            cost += np.log(np.sum( [np.exp(self.linear_model2(self.K_train[:,i], w[:,j])) for j in range(self.num_class)] )) - self.linear_model2(self.K_train[:,i], w[:,int(self.y[0,i])])\n",
    "            # cost += np.log(np.sum( [np.exp(np.dot(self.x[:,i].T, w[1:,j]) + w[0,j]) for j in range(self.num_class)] ))\n",
    "        cost = cost / len(self.y[0])\n",
    "        return cost\n",
    "\n",
    "    def GD(self, loss, lr):\n",
    "        \n",
    "        grad_fun = grad(loss)\n",
    "        self.w = self.w - lr*grad_fun(self.w)\n",
    "            \n",
    "    def test_misclassification(self):\n",
    "        misclass = 0\n",
    "        # w = self.normalize_weight(w)\n",
    "        max_class, max_dis = self.max_distance(self.w)  #w\n",
    "        for i,c in enumerate(max_class):\n",
    "            if c != self.y[0,i]:\n",
    "                misclass += 1\n",
    "        return misclass\n",
    "    \n",
    "    def test_accuracy(self):\n",
    "        misclass = self.test_misclassification()\n",
    "        return (len(self.y[0]) - misclass) / len(self.y[0])\n",
    "\n",
    "    def predict(self, image):\n",
    "        max_class = np.argmax(self.predict_model(image), axis = 1)\n",
    "        return max_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, lr_init, loss_fun, val_fun, lr_mode = 'fix'):\n",
    "    log = np.array([[loss_fun(model.w), val_fun()]])\n",
    "    for i in range(epoch):\n",
    "        if lr_mode == 'diminish':\n",
    "            lr = lr_init / (i+1)\n",
    "        elif lr_mode == 'fix':\n",
    "            lr = lr_init\n",
    "        \n",
    "        model.GD(loss_fun, lr)\n",
    "        # model.newtons_method(loss_fun)\n",
    "        loss = loss_fun(model.w)\n",
    "        val = val_fun()\n",
    "        print('finish epoch: {}, loss: {}, val: {}'.format(i,loss,val))\n",
    "        log = np.append(log, np.array([[loss, val]]),axis=0)\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RGBtoGray(data):\n",
    "    new_data = np.zeros((1, data.shape[1], data.shape[2]))\n",
    "    for im in data:\n",
    "        new_data = np.append(new_data, cv2.cvtColor(im.astype('float32'),cv2.COLOR_RGB2GRAY).reshape(1,data.shape[1], data.shape[2]), axis=0)\n",
    "    return new_data[1:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flatten(images):\n",
    "    images = images.reshape(images.shape[0], -1)\n",
    "    # images = images.reshape(-1, images.shape[0])\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.load('../Data/FaceMask_dataset/Pixel20/Correct.npy')\n",
    "data2 = np.load('../Data/FaceMask_dataset/Pixel20/Incorrect.npy')\n",
    "data3 = np.load('../Data/FaceMask_dataset/Pixel20/NoMask.npy')\n",
    "data = np.append(data1, data2, axis=0)\n",
    "data = np.append(data, data3, axis=0)\n",
    "x = Flatten(RGBtoGray(data)) / 1000 + 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[0]]*data1.shape[0]\n",
    "labels = np.append(labels, [[1]]*data2.shape[0], axis = 0)\n",
    "labels = np.append(labels, [[2]]*data3.shape[0], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test , y_train, y_test = train_test_split(x,labels, test_size = 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 911) (1, 911)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(x_train.shape[1], -1)\n",
    "y_train = y_train.reshape(y_train.shape[1], -1)\n",
    "print(x_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = x_train[:,:100]\n",
    "y_ = y_train[:,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish epoch: 0, loss: 1.723157803266879, val: 58\n",
      "finish epoch: 1, loss: 1.6741879235893422, val: 59\n",
      "finish epoch: 2, loss: 1.6291787249368468, val: 59\n",
      "finish epoch: 3, loss: 1.5880320772856953, val: 59\n",
      "finish epoch: 4, loss: 1.5506076652878944, val: 59\n",
      "finish epoch: 5, loss: 1.516729737746528, val: 58\n",
      "finish epoch: 6, loss: 1.4861948228608801, val: 58\n",
      "finish epoch: 7, loss: 1.4587797776188929, val: 58\n",
      "finish epoch: 8, loss: 1.4342496072950703, val: 57\n",
      "finish epoch: 9, loss: 1.4123646106013772, val: 58\n",
      "finish epoch: 10, loss: 1.3928865494847409, val: 55\n",
      "finish epoch: 11, loss: 1.3755836847381664, val: 56\n",
      "finish epoch: 12, loss: 1.3602346414542228, val: 56\n",
      "finish epoch: 13, loss: 1.3466311621580456, val: 57\n",
      "finish epoch: 14, loss: 1.3345798677954093, val: 59\n",
      "finish epoch: 15, loss: 1.3239031804118144, val: 59\n",
      "finish epoch: 16, loss: 1.3144395720073738, val: 59\n",
      "finish epoch: 17, loss: 1.3060432982746866, val: 59\n",
      "finish epoch: 18, loss: 1.2985837598989953, val: 59\n",
      "finish epoch: 19, loss: 1.2919446128794323, val: 60\n",
      "finish epoch: 20, loss: 1.2860227266285218, val: 60\n",
      "finish epoch: 21, loss: 1.280727066864282, val: 60\n",
      "finish epoch: 22, loss: 1.2759775609230084, val: 60\n",
      "finish epoch: 23, loss: 1.2717039867186128, val: 60\n",
      "finish epoch: 24, loss: 1.2678449132668332, val: 60\n",
      "finish epoch: 25, loss: 1.2643467102796417, val: 60\n",
      "finish epoch: 26, loss: 1.2611626364565065, val: 60\n",
      "finish epoch: 27, loss: 1.258252010336404, val: 60\n",
      "finish epoch: 28, loss: 1.2555794635140332, val: 60\n",
      "finish epoch: 29, loss: 1.2531142732904952, val: 59\n",
      "finish epoch: 30, loss: 1.2508297701018734, val: 59\n",
      "finish epoch: 31, loss: 1.2487028140849568, val: 59\n",
      "finish epoch: 32, loss: 1.2467133346879864, val: 60\n",
      "finish epoch: 33, loss: 1.2448439271531693, val: 60\n",
      "finish epoch: 34, loss: 1.2430794998638421, val: 60\n",
      "finish epoch: 35, loss: 1.2414069668713317, val: 60\n",
      "finish epoch: 36, loss: 1.2398149803290446, val: 61\n",
      "finish epoch: 37, loss: 1.238293698017287, val: 60\n",
      "finish epoch: 38, loss: 1.2368345816098598, val: 61\n",
      "finish epoch: 39, loss: 1.235430221791433, val: 61\n",
      "finish epoch: 40, loss: 1.2340741867695295, val: 60\n",
      "finish epoch: 41, loss: 1.2327608911291383, val: 60\n",
      "finish epoch: 42, loss: 1.2314854823474857, val: 60\n",
      "finish epoch: 43, loss: 1.2302437426202883, val: 61\n",
      "finish epoch: 44, loss: 1.229032003949469, val: 61\n",
      "finish epoch: 45, loss: 1.2278470747074872, val: 61\n",
      "finish epoch: 46, loss: 1.2266861761275245, val: 60\n",
      "finish epoch: 47, loss: 1.2255468873743625, val: 60\n",
      "finish epoch: 48, loss: 1.2244270980307035, val: 60\n",
      "finish epoch: 49, loss: 1.223324966990616, val: 60\n",
      "finish epoch: 50, loss: 1.2222388868882588, val: 59\n",
      "finish epoch: 51, loss: 1.2211674533085954, val: 59\n",
      "finish epoch: 52, loss: 1.2201094381294935, val: 60\n",
      "finish epoch: 53, loss: 1.2190637664335071, val: 60\n",
      "finish epoch: 54, loss: 1.2180294965045053, val: 60\n",
      "finish epoch: 55, loss: 1.2170058024907016, val: 60\n",
      "finish epoch: 56, loss: 1.2159919593729425, val: 60\n",
      "finish epoch: 57, loss: 1.2149873299265928, val: 60\n",
      "finish epoch: 58, loss: 1.213991353407977, val: 60\n",
      "finish epoch: 59, loss: 1.2130035357331597, val: 60\n",
      "finish epoch: 60, loss: 1.2120234409485284, val: 60\n",
      "finish epoch: 61, loss: 1.2110506838200223, val: 60\n",
      "finish epoch: 62, loss: 1.2100849233913928, val: 60\n",
      "finish epoch: 63, loss: 1.2091258573822432, val: 61\n",
      "finish epoch: 64, loss: 1.2081732173141095, val: 61\n",
      "finish epoch: 65, loss: 1.2072267642679857, val: 61\n",
      "finish epoch: 66, loss: 1.2062862851897096, val: 61\n",
      "finish epoch: 67, loss: 1.2053515896709366, val: 61\n",
      "finish epoch: 68, loss: 1.2044225071430883, val: 61\n",
      "finish epoch: 69, loss: 1.2034988844301162, val: 61\n",
      "finish epoch: 70, loss: 1.2025805836131185, val: 61\n",
      "finish epoch: 71, loss: 1.2016674801661404, val: 61\n",
      "finish epoch: 72, loss: 1.2007594613278827, val: 61\n",
      "finish epoch: 73, loss: 1.1998564246787515, val: 61\n",
      "finish epoch: 74, loss: 1.1989582768966893, val: 61\n",
      "finish epoch: 75, loss: 1.1980649326687707, val: 60\n",
      "finish epoch: 76, loss: 1.19717631373855, val: 60\n",
      "finish epoch: 77, loss: 1.196292348071785, val: 61\n",
      "finish epoch: 78, loss: 1.1954129691254405, val: 61\n",
      "finish epoch: 79, loss: 1.1945381152068255, val: 61\n",
      "finish epoch: 80, loss: 1.1936677289114581, val: 61\n",
      "finish epoch: 81, loss: 1.1928017566297, val: 61\n",
      "finish epoch: 82, loss: 1.191940148113517, val: 61\n",
      "finish epoch: 83, loss: 1.1910828560958162, val: 61\n",
      "finish epoch: 84, loss: 1.1902298359558037, val: 61\n",
      "finish epoch: 85, loss: 1.1893810454246116, val: 61\n",
      "finish epoch: 86, loss: 1.1885364443262219, val: 61\n",
      "finish epoch: 87, loss: 1.1876959943493115, val: 61\n",
      "finish epoch: 88, loss: 1.1868596588462128, val: 61\n",
      "finish epoch: 89, loss: 1.186027402655672, val: 61\n",
      "finish epoch: 90, loss: 1.185199191946487, val: 61\n",
      "finish epoch: 91, loss: 1.1843749940794919, val: 60\n",
      "finish epoch: 92, loss: 1.1835547774856687, val: 60\n",
      "finish epoch: 93, loss: 1.182738511558422, val: 60\n",
      "finish epoch: 94, loss: 1.1819261665583345, val: 60\n",
      "finish epoch: 95, loss: 1.181117713528899, val: 60\n",
      "finish epoch: 96, loss: 1.1803131242219167, val: 60\n",
      "finish epoch: 97, loss: 1.179512371031415, val: 60\n",
      "finish epoch: 98, loss: 1.178715426935089, val: 60\n",
      "finish epoch: 99, loss: 1.1779222654423602, val: 60\n",
      "finish epoch: 100, loss: 1.1771328605482931, val: 60\n",
      "finish epoch: 101, loss: 1.1763471866926865, val: 60\n",
      "finish epoch: 102, loss: 1.1755652187237184, val: 60\n",
      "finish epoch: 103, loss: 1.1747869318656472, val: 60\n",
      "finish epoch: 104, loss: 1.1740123016900743, val: 60\n",
      "finish epoch: 105, loss: 1.1732413040903769, val: 59\n",
      "finish epoch: 106, loss: 1.1724739152589445, val: 59\n",
      "finish epoch: 107, loss: 1.171710111666895, val: 59\n",
      "finish epoch: 108, loss: 1.1709498700459964, val: 59\n",
      "finish epoch: 109, loss: 1.1701931673725394, val: 59\n",
      "finish epoch: 110, loss: 1.169439980852947, val: 59\n",
      "finish epoch: 111, loss: 1.1686902879109238, val: 59\n",
      "finish epoch: 112, loss: 1.1679440661759743, val: 59\n",
      "finish epoch: 113, loss: 1.1672012934731355, val: 59\n",
      "finish epoch: 114, loss: 1.1664619478137914, val: 59\n",
      "finish epoch: 115, loss: 1.1657260073874594, val: 59\n",
      "finish epoch: 116, loss: 1.1649934505544153, val: 59\n",
      "finish epoch: 117, loss: 1.1642642558391034, val: 59\n",
      "finish epoch: 118, loss: 1.1635384019242072, val: 59\n",
      "finish epoch: 119, loss: 1.1628158676453397, val: 59\n",
      "finish epoch: 120, loss: 1.162096631986267, val: 59\n",
      "finish epoch: 121, loss: 1.161380674074615, val: 61\n",
      "finish epoch: 122, loss: 1.1606679731780118, val: 61\n",
      "finish epoch: 123, loss: 1.1599585087006, val: 61\n",
      "finish epoch: 124, loss: 1.1592522601799076, val: 61\n",
      "finish epoch: 125, loss: 1.1585492072840133, val: 61\n",
      "finish epoch: 126, loss: 1.1578493298089916, val: 60\n",
      "finish epoch: 127, loss: 1.157152607676593, val: 60\n",
      "finish epoch: 128, loss: 1.1564590209321572, val: 60\n",
      "finish epoch: 129, loss: 1.1557685497427002, val: 60\n",
      "finish epoch: 130, loss: 1.1550811743951972, val: 60\n",
      "finish epoch: 131, loss: 1.1543968752950022, val: 60\n",
      "finish epoch: 132, loss: 1.1537156329644158, val: 60\n",
      "finish epoch: 133, loss: 1.1530374280413782, val: 60\n",
      "finish epoch: 134, loss: 1.1523622412782648, val: 60\n",
      "finish epoch: 135, loss: 1.1516900535407901, val: 60\n",
      "finish epoch: 136, loss: 1.1510208458069897, val: 60\n",
      "finish epoch: 137, loss: 1.15035459916629, val: 60\n",
      "finish epoch: 138, loss: 1.149691294818642, val: 60\n",
      "finish epoch: 139, loss: 1.149030914073722, val: 60\n",
      "finish epoch: 140, loss: 1.1483734383501807, val: 60\n",
      "finish epoch: 141, loss: 1.1477188491749506, val: 60\n",
      "finish epoch: 142, loss: 1.1470671281825928, val: 60\n",
      "finish epoch: 143, loss: 1.1464182571146806, val: 60\n",
      "finish epoch: 144, loss: 1.1457722178192227, val: 60\n",
      "finish epoch: 145, loss: 1.1451289922501153, val: 60\n",
      "finish epoch: 146, loss: 1.1444885624666186, val: 60\n",
      "finish epoch: 147, loss: 1.143850910632862, val: 60\n",
      "finish epoch: 148, loss: 1.1432160190173664, val: 60\n",
      "finish epoch: 149, loss: 1.1425838699925863, val: 60\n",
      "finish epoch: 150, loss: 1.141954446034469, val: 60\n",
      "finish epoch: 151, loss: 1.1413277297220308, val: 60\n",
      "finish epoch: 152, loss: 1.1407037037369334, val: 60\n",
      "finish epoch: 153, loss: 1.1400823508630906, val: 60\n",
      "finish epoch: 154, loss: 1.1394636539862666, val: 60\n",
      "finish epoch: 155, loss: 1.1388475960936912, val: 60\n",
      "finish epoch: 156, loss: 1.1382341602736796, val: 60\n",
      "finish epoch: 157, loss: 1.1376233297152571, val: 60\n",
      "finish epoch: 158, loss: 1.1370150877077936, val: 60\n",
      "finish epoch: 159, loss: 1.1364094176406319, val: 60\n",
      "finish epoch: 160, loss: 1.1358063030027299, val: 59\n",
      "finish epoch: 161, loss: 1.1352057273822989, val: 59\n",
      "finish epoch: 162, loss: 1.1346076744664477, val: 59\n",
      "finish epoch: 163, loss: 1.1340121280408204, val: 59\n",
      "finish epoch: 164, loss: 1.133419071989249, val: 59\n",
      "finish epoch: 165, loss: 1.1328284902933898, val: 59\n",
      "finish epoch: 166, loss: 1.1322403670323788, val: 59\n",
      "finish epoch: 167, loss: 1.1316546863824655, val: 59\n",
      "finish epoch: 168, loss: 1.1310714326166673, val: 59\n",
      "finish epoch: 169, loss: 1.1304905901044047, val: 59\n",
      "finish epoch: 170, loss: 1.1299121433111528, val: 59\n",
      "finish epoch: 171, loss: 1.1293360767980787, val: 59\n",
      "finish epoch: 172, loss: 1.1287623752216818, val: 59\n",
      "finish epoch: 173, loss: 1.1281910233334391, val: 59\n",
      "finish epoch: 174, loss: 1.1276220059794342, val: 59\n",
      "finish epoch: 175, loss: 1.127055308100005, val: 59\n",
      "finish epoch: 176, loss: 1.126490914729368, val: 59\n",
      "finish epoch: 177, loss: 1.1259288109952612, val: 59\n",
      "finish epoch: 178, loss: 1.125368982118568, val: 59\n",
      "finish epoch: 179, loss: 1.1248114134129545, val: 59\n",
      "finish epoch: 180, loss: 1.1242560902844922, val: 58\n",
      "finish epoch: 181, loss: 1.1237029982312883, val: 58\n",
      "finish epoch: 182, loss: 1.1231521228431074, val: 57\n",
      "finish epoch: 183, loss: 1.1226034498009987, val: 57\n",
      "finish epoch: 184, loss: 1.1220569648769114, val: 57\n",
      "finish epoch: 185, loss: 1.1215126539333204, val: 57\n",
      "finish epoch: 186, loss: 1.1209705029228425, val: 57\n",
      "finish epoch: 187, loss: 1.1204304978878479, val: 57\n",
      "finish epoch: 188, loss: 1.1198926249600836, val: 57\n",
      "finish epoch: 189, loss: 1.1193568703602772, val: 57\n",
      "finish epoch: 190, loss: 1.1188232203977544, val: 58\n",
      "finish epoch: 191, loss: 1.1182916614700475, val: 58\n",
      "finish epoch: 192, loss: 1.1177621800625028, val: 58\n",
      "finish epoch: 193, loss: 1.1172347627478862, val: 57\n",
      "finish epoch: 194, loss: 1.1167093961859933, val: 57\n",
      "finish epoch: 195, loss: 1.1161860671232495, val: 57\n",
      "finish epoch: 196, loss: 1.1156647623923133, val: 57\n",
      "finish epoch: 197, loss: 1.1151454689116809, val: 57\n",
      "finish epoch: 198, loss: 1.1146281736852826, val: 57\n",
      "finish epoch: 199, loss: 1.1141128638020847, val: 57\n",
      "finish epoch: 200, loss: 1.113599526435689, val: 58\n",
      "finish epoch: 201, loss: 1.113088148843924, val: 59\n",
      "finish epoch: 202, loss: 1.11257871836845, val: 59\n",
      "finish epoch: 203, loss: 1.1120712224343456, val: 59\n",
      "finish epoch: 204, loss: 1.1115656485497092, val: 59\n",
      "finish epoch: 205, loss: 1.1110619843052478, val: 59\n",
      "finish epoch: 206, loss: 1.1105602173738727, val: 59\n",
      "finish epoch: 207, loss: 1.1100603355102892, val: 59\n",
      "finish epoch: 208, loss: 1.1095623265505903, val: 59\n",
      "finish epoch: 209, loss: 1.1090661784118494, val: 59\n",
      "finish epoch: 210, loss: 1.108571879091703, val: 59\n",
      "finish epoch: 211, loss: 1.1080794166679486, val: 59\n",
      "finish epoch: 212, loss: 1.1075887792981316, val: 59\n",
      "finish epoch: 213, loss: 1.1070999552191336, val: 58\n",
      "finish epoch: 214, loss: 1.1066129327467622, val: 58\n",
      "finish epoch: 215, loss: 1.1061277002753356, val: 58\n",
      "finish epoch: 216, loss: 1.105644246277278, val: 58\n",
      "finish epoch: 217, loss: 1.1051625593027017, val: 58\n",
      "finish epoch: 218, loss: 1.1046826279789956, val: 58\n",
      "finish epoch: 219, loss: 1.1042044410104175, val: 59\n",
      "finish epoch: 220, loss: 1.1037279871776748, val: 59\n",
      "finish epoch: 221, loss: 1.1032532553375192, val: 59\n",
      "finish epoch: 222, loss: 1.1027802344223308, val: 58\n",
      "finish epoch: 223, loss: 1.1023089134397037, val: 58\n",
      "finish epoch: 224, loss: 1.1018392814720417, val: 58\n",
      "finish epoch: 225, loss: 1.1013713276761374, val: 58\n",
      "finish epoch: 226, loss: 1.1009050412827686, val: 58\n",
      "finish epoch: 227, loss: 1.1004404115962796, val: 58\n",
      "finish epoch: 228, loss: 1.099977427994178, val: 58\n",
      "finish epoch: 229, loss: 1.0995160799267178, val: 57\n",
      "finish epoch: 230, loss: 1.0990563569164917, val: 57\n",
      "finish epoch: 231, loss: 1.0985982485580223, val: 57\n",
      "finish epoch: 232, loss: 1.098141744517351, val: 57\n",
      "finish epoch: 233, loss: 1.0976868345316293, val: 57\n",
      "finish epoch: 234, loss: 1.0972335084087117, val: 57\n",
      "finish epoch: 235, loss: 1.0967817560267468, val: 58\n",
      "finish epoch: 236, loss: 1.0963315673337732, val: 58\n",
      "finish epoch: 237, loss: 1.0958829323473054, val: 58\n",
      "finish epoch: 238, loss: 1.0954358411539393, val: 58\n",
      "finish epoch: 239, loss: 1.0949902839089374, val: 58\n",
      "finish epoch: 240, loss: 1.0945462508358297, val: 58\n",
      "finish epoch: 241, loss: 1.0941037322260092, val: 57\n",
      "finish epoch: 242, loss: 1.0936627184383267, val: 57\n",
      "finish epoch: 243, loss: 1.0932231998986923, val: 57\n",
      "finish epoch: 244, loss: 1.092785167099675, val: 57\n",
      "finish epoch: 245, loss: 1.0923486106000981, val: 57\n",
      "finish epoch: 246, loss: 1.0919135210246438, val: 57\n",
      "finish epoch: 247, loss: 1.0914798890634545, val: 57\n",
      "finish epoch: 248, loss: 1.0910477054717336, val: 57\n",
      "finish epoch: 249, loss: 1.0906169610693521, val: 57\n",
      "finish epoch: 250, loss: 1.0901876467404523, val: 57\n",
      "finish epoch: 251, loss: 1.089759753433052, val: 56\n",
      "finish epoch: 252, loss: 1.0893332721586546, val: 56\n",
      "finish epoch: 253, loss: 1.0889081939918541, val: 56\n",
      "finish epoch: 254, loss: 1.0884845100699476, val: 56\n",
      "finish epoch: 255, loss: 1.0880622115925418, val: 56\n",
      "finish epoch: 256, loss: 1.0876412898211678, val: 56\n",
      "finish epoch: 257, loss: 1.087221736078893, val: 55\n",
      "finish epoch: 258, loss: 1.0868035417499335, val: 55\n",
      "finish epoch: 259, loss: 1.086386698279273, val: 55\n",
      "finish epoch: 260, loss: 1.0859711971722728, val: 55\n",
      "finish epoch: 261, loss: 1.0855570299942976, val: 55\n",
      "finish epoch: 262, loss: 1.0851441883703286, val: 55\n",
      "finish epoch: 263, loss: 1.084732663984587, val: 55\n",
      "finish epoch: 264, loss: 1.084322448580153, val: 55\n",
      "finish epoch: 265, loss: 1.08391353395859, val: 55\n",
      "finish epoch: 266, loss: 1.0835059119795716, val: 55\n",
      "finish epoch: 267, loss: 1.0830995745605037, val: 55\n",
      "finish epoch: 268, loss: 1.082694513676154, val: 55\n",
      "finish epoch: 269, loss: 1.0822907213582784, val: 55\n",
      "finish epoch: 270, loss: 1.081888189695253, val: 55\n",
      "finish epoch: 271, loss: 1.0814869108317058, val: 55\n",
      "finish epoch: 272, loss: 1.0810868769681474, val: 55\n",
      "finish epoch: 273, loss: 1.0806880803606085, val: 55\n",
      "finish epoch: 274, loss: 1.0802905133202714, val: 55\n",
      "finish epoch: 275, loss: 1.0798941682131122, val: 55\n",
      "finish epoch: 276, loss: 1.0794990374595381, val: 55\n",
      "finish epoch: 277, loss: 1.0791051135340246, val: 55\n",
      "finish epoch: 278, loss: 1.0787123889647616, val: 55\n",
      "finish epoch: 279, loss: 1.0783208563332958, val: 55\n",
      "finish epoch: 280, loss: 1.0779305082741737, val: 56\n",
      "finish epoch: 281, loss: 1.077541337474589, val: 56\n",
      "finish epoch: 282, loss: 1.0771533366740353, val: 56\n",
      "finish epoch: 283, loss: 1.0767664986639462, val: 56\n",
      "finish epoch: 284, loss: 1.0763808162873574, val: 56\n",
      "finish epoch: 285, loss: 1.0759962824385498, val: 56\n",
      "finish epoch: 286, loss: 1.0756128900627129, val: 56\n",
      "finish epoch: 287, loss: 1.0752306321555922, val: 56\n",
      "finish epoch: 288, loss: 1.0748495017631547, val: 56\n",
      "finish epoch: 289, loss: 1.074469491981243, val: 56\n",
      "finish epoch: 290, loss: 1.0740905959552371, val: 56\n",
      "finish epoch: 291, loss: 1.0737128068797204, val: 56\n",
      "finish epoch: 292, loss: 1.073336117998139, val: 57\n",
      "finish epoch: 293, loss: 1.072960522602471, val: 57\n",
      "finish epoch: 294, loss: 1.0725860140328916, val: 57\n",
      "finish epoch: 295, loss: 1.0722125856774454, val: 57\n",
      "finish epoch: 296, loss: 1.0718402309717132, val: 57\n",
      "finish epoch: 297, loss: 1.0714689433984876, val: 57\n",
      "finish epoch: 298, loss: 1.0710987164874473, val: 57\n",
      "finish epoch: 299, loss: 1.0707295438148292, val: 57\n",
      "finish epoch: 300, loss: 1.0703614190031097, val: 57\n",
      "finish epoch: 301, loss: 1.0699943357206834, val: 57\n",
      "finish epoch: 302, loss: 1.0696282876815406, val: 57\n",
      "finish epoch: 303, loss: 1.0692632686449548, val: 57\n",
      "finish epoch: 304, loss: 1.0688992724151625, val: 57\n",
      "finish epoch: 305, loss: 1.068536292841051, val: 56\n",
      "finish epoch: 306, loss: 1.068174323815847, val: 56\n",
      "finish epoch: 307, loss: 1.067813359276804, val: 56\n",
      "finish epoch: 308, loss: 1.067453393204894, val: 56\n",
      "finish epoch: 309, loss: 1.0670944196244974, val: 56\n",
      "finish epoch: 310, loss: 1.0667364326031057, val: 56\n",
      "finish epoch: 311, loss: 1.0663794262510067, val: 56\n",
      "finish epoch: 312, loss: 1.0660233947209878, val: 56\n",
      "finish epoch: 313, loss: 1.065668332208037, val: 55\n",
      "finish epoch: 314, loss: 1.065314232949039, val: 55\n",
      "finish epoch: 315, loss: 1.0649610912224838, val: 55\n",
      "finish epoch: 316, loss: 1.0646089013481626, val: 55\n",
      "finish epoch: 317, loss: 1.0642576576868843, val: 55\n",
      "finish epoch: 318, loss: 1.0639073546401772, val: 55\n",
      "finish epoch: 319, loss: 1.0635579866499976, val: 55\n",
      "finish epoch: 320, loss: 1.0632095481984445, val: 55\n",
      "finish epoch: 321, loss: 1.06286203380747, val: 55\n",
      "finish epoch: 322, loss: 1.0625154380385955, val: 55\n",
      "finish epoch: 323, loss: 1.062169755492627, val: 55\n",
      "finish epoch: 324, loss: 1.0618249808093696, val: 55\n",
      "finish epoch: 325, loss: 1.0614811086673568, val: 55\n",
      "finish epoch: 326, loss: 1.0611381337835608, val: 55\n",
      "finish epoch: 327, loss: 1.0607960509131207, val: 55\n",
      "finish epoch: 328, loss: 1.0604548548490673, val: 55\n",
      "finish epoch: 329, loss: 1.0601145404220471, val: 55\n",
      "finish epoch: 330, loss: 1.0597751025000521, val: 55\n",
      "finish epoch: 331, loss: 1.0594365359881486, val: 55\n",
      "finish epoch: 332, loss: 1.059098835828208, val: 55\n",
      "finish epoch: 333, loss: 1.0587619969986413, val: 55\n",
      "finish epoch: 334, loss: 1.0584260145141295, val: 55\n",
      "finish epoch: 335, loss: 1.058090883425366, val: 55\n",
      "finish epoch: 336, loss: 1.057756598818787, val: 55\n",
      "finish epoch: 337, loss: 1.0574231558163176, val: 55\n",
      "finish epoch: 338, loss: 1.0570905495751095, val: 55\n",
      "finish epoch: 339, loss: 1.0567587752872807, val: 55\n",
      "finish epoch: 340, loss: 1.0564278281796669, val: 55\n",
      "finish epoch: 341, loss: 1.0560977035135637, val: 55\n",
      "finish epoch: 342, loss: 1.0557683965844704, val: 55\n",
      "finish epoch: 343, loss: 1.0554399027218448, val: 55\n",
      "finish epoch: 344, loss: 1.055112217288851, val: 55\n",
      "finish epoch: 345, loss: 1.0547853356821126, val: 55\n",
      "finish epoch: 346, loss: 1.0544592533314658, val: 55\n",
      "finish epoch: 347, loss: 1.0541339656997146, val: 55\n",
      "finish epoch: 348, loss: 1.0538094682823906, val: 55\n",
      "finish epoch: 349, loss: 1.0534857566075067, val: 55\n",
      "finish epoch: 350, loss: 1.0531628262353216, val: 55\n",
      "finish epoch: 351, loss: 1.0528406727580997, val: 55\n",
      "finish epoch: 352, loss: 1.0525192917998742, val: 55\n",
      "finish epoch: 353, loss: 1.0521986790162128, val: 55\n",
      "finish epoch: 354, loss: 1.051878830093983, val: 55\n",
      "finish epoch: 355, loss: 1.051559740751121, val: 55\n",
      "finish epoch: 356, loss: 1.0512414067364013, val: 55\n",
      "finish epoch: 357, loss: 1.0509238238292065, val: 55\n",
      "finish epoch: 358, loss: 1.0506069878392998, val: 55\n",
      "finish epoch: 359, loss: 1.0502908946066016, val: 55\n",
      "finish epoch: 360, loss: 1.0499755400009614, val: 55\n",
      "finish epoch: 361, loss: 1.0496609199219387, val: 54\n",
      "finish epoch: 362, loss: 1.0493470302985741, val: 54\n",
      "finish epoch: 363, loss: 1.0490338670891823, val: 54\n",
      "finish epoch: 364, loss: 1.048721426281121, val: 54\n",
      "finish epoch: 365, loss: 1.0484097038905795, val: 54\n",
      "finish epoch: 366, loss: 1.0480986959623626, val: 54\n",
      "finish epoch: 367, loss: 1.0477883985696783, val: 54\n",
      "finish epoch: 368, loss: 1.0474788078139208, val: 54\n",
      "finish epoch: 369, loss: 1.0471699198244624, val: 54\n",
      "finish epoch: 370, loss: 1.0468617307584451, val: 54\n",
      "finish epoch: 371, loss: 1.046554236800569, val: 54\n",
      "finish epoch: 372, loss: 1.046247434162886, val: 54\n",
      "finish epoch: 373, loss: 1.0459413190845992, val: 54\n",
      "finish epoch: 374, loss: 1.04563588783185, val: 54\n",
      "finish epoch: 375, loss: 1.0453311366975233, val: 54\n",
      "finish epoch: 376, loss: 1.0450270620010447, val: 54\n",
      "finish epoch: 377, loss: 1.044723660088177, val: 54\n",
      "finish epoch: 378, loss: 1.0444209273308274, val: 54\n",
      "finish epoch: 379, loss: 1.0441188601268456, val: 54\n",
      "finish epoch: 380, loss: 1.0438174548998311, val: 54\n",
      "finish epoch: 381, loss: 1.0435167080989356, val: 54\n",
      "finish epoch: 382, loss: 1.0432166161986771, val: 54\n",
      "finish epoch: 383, loss: 1.0429171756987394, val: 54\n",
      "finish epoch: 384, loss: 1.0426183831237903, val: 54\n",
      "finish epoch: 385, loss: 1.0423202350232856, val: 54\n",
      "finish epoch: 386, loss: 1.0420227279712861, val: 54\n",
      "finish epoch: 387, loss: 1.0417258585662719, val: 54\n",
      "finish epoch: 388, loss: 1.0414296234309526, val: 54\n",
      "finish epoch: 389, loss: 1.0411340192120884, val: 54\n",
      "finish epoch: 390, loss: 1.040839042580309, val: 54\n",
      "finish epoch: 391, loss: 1.0405446902299267, val: 54\n",
      "finish epoch: 392, loss: 1.0402509588787618, val: 54\n",
      "finish epoch: 393, loss: 1.0399578452679632, val: 54\n",
      "finish epoch: 394, loss: 1.03966534616183, val: 54\n",
      "finish epoch: 395, loss: 1.039373458347638, val: 54\n",
      "finish epoch: 396, loss: 1.0390821786354627, val: 54\n",
      "finish epoch: 397, loss: 1.0387915038580073, val: 54\n",
      "finish epoch: 398, loss: 1.0385014308704315, val: 54\n",
      "finish epoch: 399, loss: 1.038211956550178, val: 54\n",
      "finish epoch: 400, loss: 1.0379230777968076, val: 54\n",
      "finish epoch: 401, loss: 1.037634791531827, val: 54\n",
      "finish epoch: 402, loss: 1.0373470946985204, val: 54\n",
      "finish epoch: 403, loss: 1.0370599842617896, val: 53\n",
      "finish epoch: 404, loss: 1.036773457207985, val: 53\n",
      "finish epoch: 405, loss: 1.0364875105447418, val: 53\n",
      "finish epoch: 406, loss: 1.0362021413008211, val: 53\n",
      "finish epoch: 407, loss: 1.0359173465259441, val: 53\n",
      "finish epoch: 408, loss: 1.0356331232906377, val: 53\n",
      "finish epoch: 409, loss: 1.0353494686860707, val: 53\n",
      "finish epoch: 410, loss: 1.035066379823901, val: 53\n",
      "finish epoch: 411, loss: 1.034783853836114, val: 53\n",
      "finish epoch: 412, loss: 1.034501887874872, val: 53\n",
      "finish epoch: 413, loss: 1.0342204791123573, val: 53\n",
      "finish epoch: 414, loss: 1.0339396247406218, val: 53\n",
      "finish epoch: 415, loss: 1.033659321971433, val: 53\n",
      "finish epoch: 416, loss: 1.0333795680361222, val: 53\n",
      "finish epoch: 417, loss: 1.0331003601854392, val: 53\n",
      "finish epoch: 418, loss: 1.0328216956893999, val: 53\n",
      "finish epoch: 419, loss: 1.0325435718371414, val: 53\n",
      "finish epoch: 420, loss: 1.032265985936774, val: 53\n",
      "finish epoch: 421, loss: 1.0319889353152372, val: 53\n",
      "finish epoch: 422, loss: 1.031712417318154, val: 53\n",
      "finish epoch: 423, loss: 1.0314364293096911, val: 53\n",
      "finish epoch: 424, loss: 1.0311609686724146, val: 53\n",
      "finish epoch: 425, loss: 1.0308860328071474, val: 53\n",
      "finish epoch: 426, loss: 1.0306116191328343, val: 53\n",
      "finish epoch: 427, loss: 1.0303377250863983, val: 53\n",
      "finish epoch: 428, loss: 1.0300643481226066, val: 53\n",
      "finish epoch: 429, loss: 1.0297914857139319, val: 53\n",
      "finish epoch: 430, loss: 1.0295191353504156, val: 53\n",
      "finish epoch: 431, loss: 1.0292472945395361, val: 52\n",
      "finish epoch: 432, loss: 1.0289759608060742, val: 52\n",
      "finish epoch: 433, loss: 1.0287051316919795, val: 52\n",
      "finish epoch: 434, loss: 1.0284348047562386, val: 52\n",
      "finish epoch: 435, loss: 1.0281649775747461, val: 52\n",
      "finish epoch: 436, loss: 1.0278956477401744, val: 52\n",
      "finish epoch: 437, loss: 1.0276268128618442, val: 52\n",
      "finish epoch: 438, loss: 1.0273584705655967, val: 52\n",
      "finish epoch: 439, loss: 1.0270906184936697, val: 52\n",
      "finish epoch: 440, loss: 1.0268232543045674, val: 52\n",
      "finish epoch: 441, loss: 1.0265563756729383, val: 52\n",
      "finish epoch: 442, loss: 1.0262899802894523, val: 52\n",
      "finish epoch: 443, loss: 1.026024065860676, val: 52\n",
      "finish epoch: 444, loss: 1.0257586301089505, val: 52\n",
      "finish epoch: 445, loss: 1.0254936707722722, val: 52\n",
      "finish epoch: 446, loss: 1.0252291856041704, val: 52\n",
      "finish epoch: 447, loss: 1.0249651723735906, val: 52\n",
      "finish epoch: 448, loss: 1.0247016288647741, val: 52\n",
      "finish epoch: 449, loss: 1.0244385528771425, val: 52\n",
      "finish epoch: 450, loss: 1.024175942225178, val: 52\n",
      "finish epoch: 451, loss: 1.0239137947383132, val: 52\n",
      "finish epoch: 452, loss: 1.0236521082608112, val: 52\n",
      "finish epoch: 453, loss: 1.0233908806516532, val: 52\n",
      "finish epoch: 454, loss: 1.0231301097844288, val: 52\n",
      "finish epoch: 455, loss: 1.0228697935472193, val: 52\n",
      "finish epoch: 456, loss: 1.0226099298424882, val: 52\n",
      "finish epoch: 457, loss: 1.0223505165869735, val: 52\n",
      "finish epoch: 458, loss: 1.0220915517115747, val: 52\n",
      "finish epoch: 459, loss: 1.0218330331612437, val: 52\n",
      "finish epoch: 460, loss: 1.0215749588948813, val: 52\n",
      "finish epoch: 461, loss: 1.0213173268852256, val: 52\n",
      "finish epoch: 462, loss: 1.0210601351187492, val: 52\n",
      "finish epoch: 463, loss: 1.0208033815955522, val: 52\n",
      "finish epoch: 464, loss: 1.020547064329255, val: 52\n",
      "finish epoch: 465, loss: 1.020291181346903, val: 52\n",
      "finish epoch: 466, loss: 1.020035730688855, val: 52\n",
      "finish epoch: 467, loss: 1.0197807104086833, val: 52\n",
      "finish epoch: 468, loss: 1.0195261185730753, val: 52\n",
      "finish epoch: 469, loss: 1.0192719532617298, val: 52\n",
      "finish epoch: 470, loss: 1.01901821256726, val: 52\n",
      "finish epoch: 471, loss: 1.018764894595092, val: 52\n",
      "finish epoch: 472, loss: 1.0185119974633667, val: 52\n",
      "finish epoch: 473, loss: 1.0182595193028459, val: 52\n",
      "finish epoch: 474, loss: 1.0180074582568108, val: 52\n",
      "finish epoch: 475, loss: 1.0177558124809678, val: 52\n",
      "finish epoch: 476, loss: 1.0175045801433553, val: 52\n",
      "finish epoch: 477, loss: 1.0172537594242477, val: 52\n",
      "finish epoch: 478, loss: 1.0170033485160601, val: 52\n",
      "finish epoch: 479, loss: 1.0167533456232594, val: 52\n",
      "finish epoch: 480, loss: 1.0165037489622661, val: 52\n",
      "finish epoch: 481, loss: 1.0162545567613694, val: 52\n",
      "finish epoch: 482, loss: 1.0160057672606317, val: 52\n",
      "finish epoch: 483, loss: 1.0157573787118006, val: 52\n",
      "finish epoch: 484, loss: 1.0155093893782183, val: 52\n",
      "finish epoch: 485, loss: 1.0152617975347344, val: 52\n",
      "finish epoch: 486, loss: 1.0150146014676156, val: 52\n",
      "finish epoch: 487, loss: 1.01476779947446, val: 52\n",
      "finish epoch: 488, loss: 1.0145213898641123, val: 52\n",
      "finish epoch: 489, loss: 1.0142753709565717, val: 52\n",
      "finish epoch: 490, loss: 1.0140297410829158, val: 52\n",
      "finish epoch: 491, loss: 1.0137844985852076, val: 52\n",
      "finish epoch: 492, loss: 1.0135396418164135, val: 52\n",
      "finish epoch: 493, loss: 1.0132951691403245, val: 52\n",
      "finish epoch: 494, loss: 1.0130510789314688, val: 52\n",
      "finish epoch: 495, loss: 1.0128073695750321, val: 52\n",
      "finish epoch: 496, loss: 1.012564039466774, val: 52\n",
      "finish epoch: 497, loss: 1.0123210870129515, val: 52\n",
      "finish epoch: 498, loss: 1.0120785106302328, val: 52\n",
      "finish epoch: 499, loss: 1.011836308745625, val: 52\n",
      "[[ 1.01402974 52.        ]\n",
      " [ 1.0137845  52.        ]\n",
      " [ 1.01353964 52.        ]\n",
      " [ 1.01329517 52.        ]\n",
      " [ 1.01305108 52.        ]\n",
      " [ 1.01280737 52.        ]\n",
      " [ 1.01256404 52.        ]\n",
      " [ 1.01232109 52.        ]\n",
      " [ 1.01207851 52.        ]\n",
      " [ 1.01183631 52.        ]]\n"
     ]
    }
   ],
   "source": [
    "model = SVMMultiClass_Model(x_,y_,3, beta=1)\n",
    "log = train(model, 500, 0.05, model.softmax_multiclass, model.test_misclassification)\n",
    "print(log[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4141b5ec40>]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xa = range(len(log))\n",
    "f,axes = plt.subplots(1,2,figsize=(10,5))\n",
    "axes[0].plot(xa,log[:,0])\n",
    "axes[1].plot(xa,log[:,1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd876119c40028cab099c61ef2a2b5818981311f73eb916c96f6645cbc4fa9aa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('ML': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
