{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import hessian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel:\n",
    "    def __init__(self,name,**kwargs):         \n",
    "        # define kernel            \n",
    "        if name == 'polys':\n",
    "            self.kernel = self.kernel_poly\n",
    "            \n",
    "        if name == 'fourier':\n",
    "            self.kernel = self.kernel_fourier   \n",
    "\n",
    "        if name == 'gaussian':\n",
    "            self.kernel = self.kernel_gaussian   \n",
    "            \n",
    "        ### set hyperparameters of kernel ###\n",
    "        # degree of polynomial\n",
    "        self.D = 2\n",
    "        if 'degree' in kwargs:\n",
    "            self.D = kwargs['degree']\n",
    "            \n",
    "        self.beta = 0.0001\n",
    "        if 'beta' in kwargs:\n",
    "            self.beta = kwargs['beta']\n",
    "            \n",
    "    # poly kernel\n",
    "    def kernel_poly(self,x1,x2):    \n",
    "        H = (1 + np.dot(x1.T,x2))**self.D - 1\n",
    "        return H.T\n",
    "    \n",
    "    # fourier kernel\n",
    "    def kernel_fourier(self,x1,x2):    \n",
    "        # loop over both matrices and create fourier kernel\n",
    "        num_cols1 = x1.shape[1]\n",
    "        num_cols2 = x2.shape[1]\n",
    "        H = np.zeros((num_cols1,num_cols2))\n",
    "        for n in range(num_cols1):\n",
    "            for m in range(num_cols2):\n",
    "                val = np.pi*(x1[:,n] - x2[:,m])                \n",
    "                ind = np.argwhere(val == 0)\n",
    "                val[ind] += 10**(-10)\n",
    "                val1 = np.sin((2*self.D + 1)*val)\n",
    "                val2 = np.sin(val)\n",
    "                val3 = np.prod(val1/val2,0) - 1\n",
    "                H[n,m] = val3\n",
    "        return H.T\n",
    "    \n",
    "    # gaussian kernel\n",
    "    def kernel_gaussian(self,x1,x2):  \n",
    "        dist = cdist(x1.T, x2.T, metric='euclidean')**2\n",
    "        H = np.exp(-self.beta*dist)\n",
    "        return H.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load(path, norm=False):\n",
    "    data = np.loadtxt(path, delimiter=',')\n",
    "    print(data.shape)\n",
    "    data = clean(data)\n",
    "    x = data[:-1, :]\n",
    "    if norm is True:\n",
    "        x = normalize(x)\n",
    "    y = data[-1:,:]\n",
    "    return x,y\n",
    "\n",
    "def normalize(x):\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        mean = np.mean(x[i])\n",
    "        dev = np.sqrt(np.mean(np.square(x[i]-mean)))\n",
    "        x[i] = (x[i] - mean) / dev\n",
    "    return x\n",
    "\n",
    "def clean(data):\n",
    "    return data[:,~np.isnan(data).any(axis=0)]  # delete all the coloum which has nan\n",
    "\n",
    "\n",
    "class SVMMultiClass_Model:\n",
    "    def __init__(self, x, y, num_class, kernel_name = 'gaussian'):       # x: [features, batch]  y: [1, batch]\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.num_class = num_class\n",
    "        self.kernel = Kernel(kernel_name)\n",
    "        self.K_train = self.kernel.kernel(self.x,self.x)\n",
    "        self.w = np.random.random((len(self.K_train)+1,num_class))*2 - 1       # w: [features, class]\n",
    "\n",
    "\n",
    "        pass\n",
    "    def linear_model(self, w):\n",
    "        return np.dot(self.K_train.T, w[1:,:]) + w[0,:]      # [batch, class]\n",
    "\n",
    "    def linear_model2(self,x,w):                       \n",
    "        return np.dot(x.T, w[1:]) + w[0]\n",
    "    \n",
    "\n",
    "    def normalize_weight(self, w):\n",
    "        '''\n",
    "        nofrmailze the weight for signed distance\n",
    "        w_normalized = w_origin / sqrt(sum(w_origin^2)) for each class\n",
    "        '''   \n",
    "        for c in range(len(w[0])):    # loop class\n",
    "            sum = np.sqrt(np.sum(np.square(w[:,c])))   # L2 norm\n",
    "            w[:,c] = w[:,c] / sum\n",
    "        return w\n",
    "\n",
    "    def max_distance(self, w):\n",
    "        max_class = np.argmax(self.linear_model(w), axis = 1)     #[batch] \n",
    "        max_dis = np.max(self.linear_model(w), axis = 1)          #[batch]\n",
    "        return max_class, max_dis\n",
    "\n",
    "    def perceptron_multiclass(self, w):\n",
    "        # w = self.normalize_weight(w)\n",
    "        cost = 0\n",
    "        max_class, max_dis = self.max_distance(w)\n",
    "        for i in range(len(self.x[0])):\n",
    "            cost += max_dis[i] - (np.dot(self.x[:,i], w[1:,int(self.y[0,i])]) + w[0, int(self.y[0,i])])\n",
    "        cost = cost / len(self.x[0])\n",
    "        return cost\n",
    "    \n",
    "    def softmax_multiclass(self, w):\n",
    "        cost = 0\n",
    "        for i in range(len(self.y[0])):\n",
    "            cost += np.log(np.sum( [np.exp(self.linear_model2(self.K_train[:,i], w[:,j])) for j in range(self.num_class)] )) - self.linear_model2(self.K_train[:,i], w[:,int(self.y[0,i])])\n",
    "            # cost += np.log(np.sum( [np.exp(np.dot(self.x[:,i].T, w[1:,j]) + w[0,j]) for j in range(self.num_class)] ))\n",
    "        cost = cost / len(self.y[0])\n",
    "        return cost\n",
    "\n",
    "    def GD(self, loss, lr):\n",
    "        \n",
    "        grad_fun = grad(loss)\n",
    "        self.w = self.w - lr*grad_fun(self.w)\n",
    "            \n",
    "    def test_misclassification(self):\n",
    "        misclass = 0\n",
    "        # w = self.normalize_weight(w)\n",
    "        max_class, max_dis = self.max_distance(self.w)  #w\n",
    "        for i,c in enumerate(max_class):\n",
    "            if c != self.y[0,i]:\n",
    "                misclass += 1\n",
    "        return misclass\n",
    "    \n",
    "    def test_accuracy(self):\n",
    "        misclass = self.test_misclassification()\n",
    "        return (len(self.y[0]) - misclass) / len(self.y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMmodel:\n",
    "    def __init__(self, x, y, kernel_name= 'gaussian'):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.kernel = Kernel(kernel_name)\n",
    "        self.K_train = self.kernel.kernel(self.x,self.x)\n",
    "        self.w = np.random.random((len(self.K_train)+1,1))*2 - 1       # w: [features, class]\n",
    "        pass\n",
    "    \n",
    "    def linear_model(self, w):\n",
    "        return np.dot(self.K_train.T, w[1:,:]) + w[0,:]      # [batch, 1]\n",
    "\n",
    "    def logistic_regression(self, w):\n",
    "        out = 1.0 / (1.0 + np.exp(-1.0 * self.linear_model(w)))    \n",
    "        return out                                      # [batch, 1]\n",
    "    \n",
    "    def sigmod(self, w):\n",
    "        return np.log( 1.0 + np.exp((-1) * self.y.T * self.linear_model(w)))      # [batch, 1]\n",
    "\n",
    "    def perceptron(self, w):\n",
    "        return np.mean(np.maximum(np.zeros((len(self.y.T),1)), (-1) * self.y.T * self.linear_model(w)))\n",
    "    \n",
    "    def softmax(self, w):\n",
    "        return np.mean(np.log( 1 + np.exp((-1) * self.y.T * self.linear_model(w)) ))\n",
    "\n",
    "    def test_misclassification(self):\n",
    "        out = self.logistic_regression(self.w)\n",
    "        r = (out-0.5) * self.y.T\n",
    "        return np.sum(r<0)\n",
    "    \n",
    "    def test_accuracy(self):\n",
    "        out = self.logistic_regression(self.w)\n",
    "        r = (out-0.5) * self.y.T\n",
    "        return np.sum(r>0) / len(r)\n",
    "\n",
    "    def test_balanced_accuracy(self):\n",
    "        out = self.logistic_regression(self.w)\n",
    "        results = (out-0.5) * self.y.T\n",
    "\n",
    "        misclass_num = np.zeros(2)\n",
    "        positive_sum = np.sum(self.y>0)\n",
    "        negative_sum = np.sum(self.y<0)\n",
    "        for i, r in enumerate(results[:,0]<0):\n",
    "            if r==True:\n",
    "                if self.y.T[i] > 0:\n",
    "                    misclass_num[0] += 1\n",
    "                else:\n",
    "                    misclass_num[1] += 1\n",
    "        return np.mean([(positive_sum-misclass_num[0])/positive_sum, (negative_sum-misclass_num[1])/negative_sum])\n",
    "    \n",
    "    def confusion_matrix(self):\n",
    "        out = self.logistic_regression(self.w) - 0.5\n",
    "        confusion = np.zeros((2,2))\n",
    "        for i, l in enumerate(self.y[0]):\n",
    "            if l > 0:\n",
    "                if out[i] * l > 0:\n",
    "                    confusion[1,1] += 1\n",
    "                elif out[i] * l < 0:\n",
    "                    confusion[1,0] += 1\n",
    "            elif l<0:\n",
    "                if out[i] * l > 0:\n",
    "                    confusion[0,1] += 1\n",
    "                elif out[i] * l < 0:\n",
    "                    confusion[0,0] += 1\n",
    "        return confusion\n",
    "\n",
    "\n",
    "    def GD(self, loss, lr):\n",
    "        \n",
    "        grad_fun = grad(loss)\n",
    "        self.w = self.w - lr*grad_fun(self.w)\n",
    "\n",
    "    def newtons_method(self, loss_fun):\n",
    "        # update the weight using newtons_method on loss_function\n",
    "\n",
    "        # compute gradient/ Hessian using autograd\n",
    "        gradient = grad(loss_fun)\n",
    "        hess = hessian(loss_fun)\n",
    "\n",
    "        # set numerical stability parameter\n",
    "        epsilon = 10**(-7)\n",
    "\n",
    "        # evaluate the gradient and hessian\n",
    "        grad_eval = gradient(self.w)\n",
    "        # print('grad_eval', grad_eval)\n",
    "        hess_eval = hess(self.w)\n",
    "        # print('hess_eval',hess_eval)\n",
    "\n",
    "        # reshape hessian to square matrix\n",
    "        hess_eval .shape = (int((np.size(hess_eval))**(0.5)), int((np.\n",
    "                                                            size(hess_eval))**(0.5)))\n",
    "        # print(hess_eval)\n",
    "\n",
    "        # solve second -order system for weight update\n",
    "        A = hess_eval + epsilon*np.eye(self.w.size)\n",
    "        b = grad_eval\n",
    "        self.w = np.linalg.solve(A, np.dot(A, self.w)-b)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, lr_init, loss_fun, val_fun, lr_mode = 'fix'):\n",
    "    log = np.array([[loss_fun(model.w), val_fun()]])\n",
    "    for i in range(epoch):\n",
    "        if lr_mode == 'diminish':\n",
    "            lr = lr_init / (i+1)\n",
    "        elif lr_mode == 'fix':\n",
    "            lr = lr_init\n",
    "        \n",
    "        model.GD(loss_fun, lr)\n",
    "        # model.newtons_method(loss_fun)\n",
    "\n",
    "        log = np.append(log, np.array([[loss_fun(model.w), val_fun()]]),axis=0)\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 40)\n",
      "[[0.6106109  8.        ]\n",
      " [0.61015403 8.        ]\n",
      " [0.6096993  8.        ]\n",
      " [0.60924671 8.        ]\n",
      " [0.60879623 8.        ]\n",
      " [0.60834786 8.        ]\n",
      " [0.60790158 9.        ]\n",
      " [0.60745738 9.        ]\n",
      " [0.60701523 9.        ]\n",
      " [0.60657513 9.        ]]\n"
     ]
    }
   ],
   "source": [
    "x,y = load('../Data/4class_data.csv')\n",
    "model = SVMMultiClass_Model(x,y,4, kernel_name='polys')\n",
    "log = train(model, 300, 0.1, model.softmax_multiclass, model.test_misclassification)\n",
    "print(log[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 99)\n"
     ]
    }
   ],
   "source": [
    "x,y = load('../Data/new_circle_data.csv')\n",
    "# for i in range(len(y[0])):\n",
    "#     if y[0,i] == -1:\n",
    "#         y[0,i]=0\n",
    "# print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# model = SVMMultiClass_Model(x,y,4, kernel_name='polys')\n",
    "model = SVMmodel(x,y)\n",
    "out = np.sign(model.linear_model(model.w))\n",
    "print(out)\n",
    "log = train(model, 50, 0.01, model.softmax, model.test_misclassification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]]\n"
     ]
    }
   ],
   "source": [
    "out = np.sign(model.linear_model(model.w))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.54209029 23.        ]\n",
      " [ 0.54209024 23.        ]\n",
      " [ 0.54209021 23.        ]\n",
      " [ 0.54209018 23.        ]\n",
      " [ 0.54209017 23.        ]\n",
      " [ 0.54209016 23.        ]\n",
      " [ 0.54209015 23.        ]\n",
      " [ 0.54209015 23.        ]\n",
      " [ 0.54209014 23.        ]\n",
      " [ 0.54209014 23.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(log[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff4b9dc4ee0>]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f,ax = plt.subplots()\n",
    "ax.plot(range(len(log)), log[:,0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd876119c40028cab099c61ef2a2b5818981311f73eb916c96f6645cbc4fa9aa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('ML': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
